{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Coursework 2 - Student Number 200546665_CW2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classifiers [7 marks total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "from sklearn import datasets \n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris() # load data \n",
    "#print(iris.DESCR) # print dataset description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset pre-processing \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Adding the iris dataset into a dataframe\n",
    "iris_dataset = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "\n",
    "# Adding the target column to my newly created dataset\n",
    "iris_dataset['target'] = iris.target\n",
    "iris_dataset\n",
    "\n",
    "# Placing dataset (first 4 columns) and target (last column) in X and y respectively \n",
    "X = iris_dataset.iloc[:,0:4].values\n",
    "y = iris_dataset.iloc[:,4].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Na√Øve Bayes Classifier [2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write your code here      \n",
    "\n",
    "# Importing the necessary libraries\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset and the target in train and test datasets, for X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a Gaussian Naive Bayes classifier and training it on the data\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_NB = NB.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Random Forest Classifier [3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "importance \t feature name\n",
      "0.46 \t\t feature 2\n",
      "0.423 \t\t feature 3\n",
      "0.097 \t\t feature 0\n",
      "0.02 \t\t feature 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Write your code here\n",
    "\n",
    "# Importing the necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a random forest classifier and training it on the data\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_RF = RF.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_RF))\n",
    "\n",
    "# Evaluating feature performance of the model:\n",
    "\n",
    "# Getting the features names\n",
    "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Getting the features importances\n",
    "importances = RF.feature_importances_\n",
    "\n",
    "# Printing out an ordered list of feature importances\n",
    "# First, Getting our features and weights\n",
    "feature_importance = sorted( zip(importances, feature_names), reverse=True)\n",
    "# Printing\n",
    "print('importance \\t feature name')\n",
    "print(\"\\n\".join(['{} \\t\\t {}'.format(round(i,3),f) for i,f in feature_importance]))\n",
    "\n",
    "# Visualising the feature importances:\n",
    "\n",
    "# First putting the importances into and ndarray\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# Getting the standard deviation of accumulation of the impurity decrease within each tree\n",
    "Standard_Deviation = np.std([tree.feature_importances_ for tree in RF.estimators_], axis=0)\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=Standard_Deviation, ax=ax)\n",
    "ax.set_title(\"Feature importances using Mean decrease in impurity\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 kNN Classifier [2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "\n",
    "# Importing the necessary libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Creating a kNN classifier, with 1 neighbour and training it on the data\n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_KNN = KNN.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_KNN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Code Report [6 marks total]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "Firstly, to make the dataset ready for the machine learning models, I convert the iris dataset (with the 4 feature names) into a dataframe and then append the column target at the end of it. The next step will be to make sure the dataset with the first 4 feature columns is placed in object X, while the column target is placed in object y. I then use the train_test_split tool, from the sklearn model selection package, to split the dataset and the target into train and test datasets. The test size is selected at 20% and the random state is fixed at 10 (this simply sets a seed to the random generator).\n",
    "\n",
    "For the Na√Øve Bayes Classifiers, the Gaussian Na√Øve Bayes (GaussianNB) package is imported from the sklearn na√Øve bayes package. This is a variant of Naive Bayes that follows Gaussian normal distribution; I chose this model because it is appropriate for continuous data. The model is first trained into the train dataset (X_train) and into the train target (y_train); then it is predicted on the test data. The evaluation of the model is done using the package metrics imported from the sklearn metrics library, where the test target is evaluated against the predictions.\n",
    "\n",
    "For the Random Forest Classifier, the RandomForestClassifier model is first imported. Random forest uses decision trees as its base predictor and also uses bagging; it is suitable to solve both regression and classification problems.\n",
    "In my code, the n_estimators is set at a default of 100, which indicates the number of decision trees that will be used in the random forest. Again, the model is first trained, then predicted on the test data and finally evaluated.\n",
    "To evaluate the feature performance of the model, I first get all of the features' names, then fit the random forest model into the feature importances which are provided by the fitted attribute feature_importances_. This, is calculated as the mean and standard deviation of accumulation of the impurity decrease within each tree.\n",
    "I then print the ordered list of feature importances, to review which are the most important.\n",
    "Finally, I visualise the feature importances in a bar graph. First, I transfer the feature importances into a numpy array and then I calculate the standard deviation of accumulation of the impurity decrease within each tree. I use matplotlib to plot the impurity-based importance, with the feature on the x axis and the mean decrease in impurity on the y axis.\n",
    "\n",
    "For the KNN Classifier, the KNeighborsClassifier package is imported and the number of neighbours is set at 1. \n",
    "KNN is an algorithm suitable for both regression and classification tasks and it essentially forms a majority vote between the K most similar instances to a given ‚Äúunseen‚Äù observation. A new observation is classified by a majority vote of its neighbors, with the observation being assigned to the class most common amongst its K nearest neighbors, measured by a distance function.\n",
    "Again, the model is then trained, predicted and evaluated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Questions [14 marks total]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Na√Øves Bayes Questions [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "1- Let's start by defining what the zero probabilities issue in a Naive Bayes model is. The zero probabilities issue in a Na√Øve Bayes model occurs when an instance in the test data set has a category that was not present during training: the model will then assign it ‚Äú0‚Äù probability and will not be able to make a prediction. \n",
    "In other words, if an individual class label is missing, then the frequency-based probability estimate will be zero. \n",
    "This issue raises issues because it impacts the output when all the likelihoods are multiplied (the output will be zero) causing the whole performance of the classification to be skewed and the model to be less accurate.\n",
    "\n",
    "\n",
    "2- We can avoid the problem of zero probabilities by simply adding number ‚Äú1‚Äù to the count for every feature value-class combination when a feature value doesn‚Äôt occur with every class value.\n",
    "We are basically increasing the count of the variable with zero to a small value (usually 1) in the numerator, so that the overall probability doesn‚Äôt become zero.\n",
    "This will lead to the removal of all the zero values from the classes and, at the same time, will not impact the overall relative frequency of the classes.\n",
    "\n",
    "For example, let‚Äôs suppose the training data looks like this:\n",
    "\n",
    "|               | Setosa  | Versicolour  | Virginica  |\n",
    "|---------------|---------|--------------|------------|\n",
    "| Sepal length  | 10      | 5            | 8          |\n",
    "| Sepal width   | 0       | 0            | 0          |\n",
    "|               |         |              |            |\n",
    "\n",
    "P(Sepal = length | Setosa) = 10/10 =1\n",
    "\n",
    "P(Sepal = width | Setosa) = 0/10 =0\n",
    "\n",
    "\n",
    "To fix the ‚Äú0‚Äù, we should add one to every value in this table when calculating the probabilities:\n",
    "\n",
    "|               | Setosa  | Versicolour  | Virginica  |\n",
    "|---------------|---------|--------------|------------|\n",
    "| Sepal length  | 11      | 6            | 9          |\n",
    "| Sepal width   | 1       | 1            | 1          |\n",
    "|               |         |              |            |\n",
    "\n",
    "P(Sepal = length | Setosa) = 11/12 = 0.92\n",
    "\n",
    "P(Sepal = width | Setosa) = 1/12 = 0.08\n",
    "\n",
    "As we can see by the results, the zero probability has been resolved.\n",
    "\n",
    "Additionally, this is how I will avoid the problem with a pseudocode:\n",
    "- Defining the probability of each of the classes.\n",
    "- We end up with probabilities, where an element is the probability of feature f having value v, given the class c.\n",
    "- For every probability where we have counts of zero in the numerator, we add an alpha parameter or smoothing parameter which is ‚â† 0: this will solve the problem of zero probabilities in our Na√Øve Bayes model.\n",
    "- We use the probabilities calculated above to compute the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest Questions [6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "1- With an importance (or a mean decrease in impurity) calculated at 0.46, the most important feature from my random forest classifier, is feature 2: \"petal length\" (as also shown in the sorted feature importance table and visualized in the bar graph, coded above). See below a comprehensive summary:\n",
    "\n",
    "| Importance  |  Feature name              |   |\n",
    "|-------------|----------------------------|---|\n",
    "| 0.46        |  feature 2 > petal length  |   |\n",
    "| 0.423       |  feature 3 > petal width   |   |\n",
    "| 0.097       |  feature 0 > sepal length  |   |\n",
    "| 0.02        |  feature 1 > sepal width   |   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2- Starting from the fact that with all 4 features included in my random forest classifier, the accuracy (and all the other metrics) are calculated as a perfect 1.00, to increase accuracy of the model (or, more specifically, in this case, to maintain the same level of perfect accuracy) both the features with less importance \"sepal length\" & \"sepal width\" can be removed.\n",
    "\n",
    "The code below, shows the changes in the evaluation metrics when:\n",
    "\n",
    "a) the feature \"sepal width\" is removed from the dataset;\n",
    "\n",
    "b) the feature \"sepal length\" is removed from the dataset;\n",
    "\n",
    "c) both the features \"sepal width\" & \"sepal length\" are removed from the dataset.\n",
    "\n",
    "As outlined in the code below, for each, newly formed detaset, I first split the dataset into new train and test sets, then train the random forest classifier, predict and finally evaluate the new model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "# Removing feature/column \"sepal width\" from the dataset\n",
    "X_1 = iris_dataset[['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "X_1\n",
    "\n",
    "# Splitting the new dataset and the target in train and test datasets, for X_1 and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a random forest classifier and training it on the data\n",
    "RF_1 = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "RF_1.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_RF_1= RF_1.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_RF_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "# Removing feature/column \"sepal length\" from the dataset\n",
    "\n",
    "X_2 = iris_dataset[['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "X_2\n",
    "\n",
    "# Splitting the new dataset and the target in train and test datasets, for X_2 and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a random forest classifier and training it on the data\n",
    "RF_2 = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "RF_2.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_RF_2 = RF_2.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_RF_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# c)\n",
    "# Removing both columns \"sepal width\" & \"sepal length\"\n",
    "\n",
    "X_3 = iris_dataset[['petal length (cm)', 'petal width (cm)']]\n",
    "X_3\n",
    "\n",
    "# Splitting the new dataset and the target in train and test datasets, for X_3 and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_3,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a random forest classifier and training it on the data\n",
    "RF_3 = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "RF_3.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_RF_3 = RF_3.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_RF_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the evaluation results for each of the newly formed datasets (with focus on the model accuracy), the below table has been created.\n",
    "\n",
    "\n",
    "|                                                      | Model Accuracy |   |\n",
    "|------------------------------------------------------|----------|---|\n",
    "| All 4 features included                              | 1.00     |   |\n",
    "| Without feature \"sepal width\"                        | 0.97     |   |\n",
    "| Without feature \"sepal length\"                       | 0.97     |   |\n",
    "| Without both features \"sepal length\" & \"sepal width\" | 1.00     |   |\n",
    "\n",
    "As we can see, removing EITHER feature \"sepal width\" OR feature \"sepal length\" actually decrease the accuracy of the model!\n",
    "While removing BOTH these low importance features increase and mantains the accuracy (and all the other metrics) at a perfect score 1.00.\n",
    "\n",
    "\n",
    "\n",
    "3- It would be useful to remove these features because, overall, the more trustworthy the computed importances are, the more accurate the model is.\n",
    "Leaving less important features in the dataset may cause issues in terms of model performance.\n",
    "As we know, random forests consist of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset so that similar response values end up in the same set. Impurity is the measure based on which the local optimal condition is chosen. So, when training a tree, it can be computed how much each feature decreases the weighted impurity in a tree: you want to select only the important features which optimize the decrease of impurity so that when each individual tree is trained on a random subset of the features and when trees are combined, there is little correlation between them, reducing the risk of over-fitting and the risk of building dependencies between trees.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 kNN Questions [4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "1- I think the iris dataset is well suited to the kNN classifier.\n",
    "As we know, the kNN algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in many industries.\n",
    "KNN performs better with a lower number of features and it proves to be very sensitive to the scale of the dataset as it can be influenced negatively by irrelevant features.\n",
    "We can say that when the number of features increases then more data is required, leading towards an increase in dimension which can consequently lead to the problem of overfitting.\n",
    "The Iris Flower Dataset is a dataset made of 150 observations under 5 features: Petal Length, Petal Width, Sepal Length, Sepal width and Class (or Species). There are 3 classes, making this a multiclass classification problem as it involves predicting the flower species, given measurements of iris flowers.\n",
    "The Iris dataset appears to be suited to KNN, because it is a clean and fairly small classification dataset, with the three classes (setosa, virginica, and versicolor) being perfectly balanced. \n",
    "The features are only 4 and are all continuous, which also makes measuring the Euclidean distances in the kNN and calculating the accuracy much simpler. \n",
    "\n",
    "\n",
    "2- The ideal dataset will have 2 or more classes, making it a binary or multiclass classification problem.\n",
    "The dataset would be fairly small with lower dimensionality (not more than 5 features, < 200 observations) display no missing data, balanced and contain only continuous observations.\n",
    "Ideally, we want features which are more relevant to the class and less relevant to other features (minimum correlation).\n",
    "Since kNN calculates the Euclidean distance between points to determine the nearest neighbouring points, it is important that all of the data is also on the same scale. \n",
    "This could be achieved by normalizing the dataset, for example, in range 0-1. Not only this is useful when calculating the distance measures, as kNN uses also feature values, when one feature values is larger than the others, that feature can dominate the distance and affect the outcome of the KNN, so normalizing the dataset becomes really important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Comparing Models [18 marks total]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Compare each model [3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "The main differences I see between the 3 classifiers is in regards to the evaluation metrics: for all 4 features, while both the Na√Øve Bayes and the Random Forest Classifier score perfect accuracy, precision, recall and f1-score of 1.00 for all classes, the KNN scores 1.00 in precision, recall and f1-score for class 0, but only 0.92 and 0.96 in recall and f1-score for class 1 and only 0.88 in precision and 0.93 in f1-score for class 2.\n",
    "An accuracy of 0.97 makes this model the less accurate of the three.\n",
    "More specifically, only 88% of precision (which is the ability of the classifier not to label an instance positive that is actually negative) means that only 88% of predictions were correct for class 2.\n",
    "The recall (which explains what percent of the positive cases were actually caught) shows that 92% positive instances were found for class 1.\n",
    "Finally, the f1-score (which shows what percent of positive predictions were correct) is 96% for class 1 and 93% for class 2.\n",
    "\n",
    "These results show that the Na√Øve Bayes and Random Forest Classifiers are best suited for this dataset, compared to the kNN classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Accuracy [6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "Both the Na√Øve Bayes and the Random Forest Classifier score perfect accuracy of 1.00, While the kNN classifier scores 0.97 in accuracy making it the less accurate model of the three.\n",
    "The kNN classifier and the relative accuracy, could be affected by the fact that the dataset is not normalized so the features with biggest Euclidean distance, may influence the performance of the model. \n",
    "Alternatively, it could be the case that some features are irrelevant and affect the performance of the model negatively.\n",
    "\n",
    "I want to test both these 2 hypotheses.\n",
    "\n",
    "1-\tIn the code below, I have used the MinMaxScaler from Sci-Kit Learn to scale the dataset from 0 to 1. I have then split the dataset and applied the kNN classifier on the new dataset, keeping the number of neighbours at 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0           0.222222          0.625000           0.067797          0.041667\n",
       "1           0.166667          0.416667           0.067797          0.041667\n",
       "2           0.111111          0.500000           0.050847          0.041667\n",
       "3           0.083333          0.458333           0.084746          0.041667\n",
       "4           0.194444          0.666667           0.067797          0.041667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the iris dataset\n",
    "\n",
    "# Importing the necessary library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Adding the iris dataset into a dataframe\n",
    "iris_dataset = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "\n",
    "# Scaling the first 4 features columns of the iris dataset\n",
    "iris_scaled = pd.DataFrame(scaler.fit_transform(iris_dataset), columns=iris_dataset.columns)\n",
    "iris_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Placing the scaled dataset and the target in X and y respectively \n",
    "X = iris_scaled\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the dataset and the target in train and test datasets, for X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a kNN classifier, with 1 neighbour and training it on the data\n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_KNN = KNN.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_KNN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the dataset, does not improve the performance of the kNN model. The results in the evaluation and the accuracy are the same as in with the non-scaled dataset.\n",
    "\n",
    "Let's try the second hypothesis now: removing the less important features.\n",
    "\n",
    "2- I will be removing the previously calculated less important features.\n",
    "In more detail, in the code below I have:\n",
    "\n",
    "a) removed the feature \"sepal width\" from the dataset & re-trained the kNN classifier, predicted and evaluated the model;\n",
    "\n",
    "b) removed the feature \"sepal length\" from the dataset & re-trained the kNN classifier, predicted and evaluated the model;\n",
    "\n",
    "c) removed both the features \"sepal width\" & \"sepal length\" from the dataset & re-trained the kNN classifier, predicted and evaluated the model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "# Removing feature/column \"sepal width\" from the dataset\n",
    "X_1 = iris_dataset[['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "X_1\n",
    "\n",
    "# Splitting the new dataset and the target in train and test datasets, for X_1 and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a kNN classifier, with 1 neighbour and training it on the data\n",
    "KNN_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN_1.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_KNN_1 = KNN_1.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_KNN_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "# Removing feature/column \"sepal length\" from the dataset\n",
    "X_2 = iris_dataset[['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "X_2\n",
    "\n",
    "# Splitting the new dataset and the target in train and test datasets, for X_2 and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a kNN classifier, with 1 neighbour and training it on the data\n",
    "KNN_2 = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN_2.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_KNN_2 = KNN_2.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_KNN_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# c)\n",
    "# Removing both features/columns \"sepal width\" & \"sepal length\"\n",
    "X_3 = iris_dataset[['petal length (cm)', 'petal width (cm)']]\n",
    "X_3\n",
    "\n",
    "# Splitting the new dataset and the target in train and test datasets, for X_3 and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_3,y,test_size = .20, random_state = 10)\n",
    "\n",
    "# Creating a kNN classifier, with 1 neighbour and training it on the data\n",
    "KNN_3 = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN_3.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_KNN_3 = KNN_3.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(metrics.classification_report(y_test, y_pred_KNN_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the second less important feature \"sepal length\" from the dataset, improves the performance of the model, gaining a perfect accuracy score of 1.00.\n",
    "\n",
    "While, removing all other features cause no changes to the performance of the model: the accuracy and the other evaluation metrics remain unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Appropriate Use [9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write your answer here\n",
    "\n",
    "- The Na√Øve Bayes Classifier is appropriate to use with text classification problems that include a high-dimensional training dataset.\n",
    "This algorithm is popular for multi-class predictions as well. Some popular real-world examples of Na√Øve Bayes Algorithm use are spam filtration, sentimental analysis (which refers to the identification of positive or negative sentiments of a target group) or articles classification.\n",
    "In the case study of spam detection with Naive Bayes by Fernando, L. (2022), on a dataset of 4327 emails, a naive bayes classifier is used to classify a given email as spam or ham depending on its content.\n",
    "Overall, using a total of 170 Features the classifier achieves a success rate of about 94.80% being able to correctly categorize spam emails 95/100 times.\n",
    "Na√Øve Bayes Classifier is an appropriate model for this use-case, because the dataset is a text (non-numeric) and it is quite high in dimensionality. This is a binary classification problem, where this algorithm performs pretty well.\n",
    "\n",
    "\n",
    "- Random Forest Classifier is appropriate to use for both classification and regression tasks.\n",
    "It can handle a large proportion of data with higher dimensionality, while maintaining the accuracy and managing missing values.\n",
    "The more random the forest training in terms of bootstrapping training data, and the more random the feature selection is, the better the algorithm will perform. \n",
    "Random forests are computationally advantageous because of bagging: they allow each classifier to operate in parallel; and because they're based on the fast decision tree algorithm.\n",
    "There are many real-world situations in which Random Forest can be used. A few examples are: the banking industry for credit card fraud detection, in the healthcare and medicine for breast cancer prediction or in the stock market for bitcoin price detection or even in the e-commerce for product recommendation.\n",
    "In more detail, let‚Äôs analyse the credit card fraud detection example. In this project by MansiMeena (2013), Random Forest was used to identify fraud in European credit card transactions. The dataset used, is hosted on Kaggle and is made up of transactions made in September 2013. The dataset contains 284,807 transactions that occurred over a two-day period. Of these, 492 (0.17%) are indeed fraudulent. Each transaction has 30, all numerical, features.\n",
    "Random Forest is an appropriate model for this use-case, because this a binary classification problem where the dataset is particularly large and with very high dimensionality. In the project, it was possible to accurately identify fraudulent transactions: on a 20% test set, the predictions from the random forest model had an F1 score of 0.869: a better result compared to other models!\n",
    "\n",
    "\n",
    "\n",
    "- kNN Classifier can be used for both classification and regression predictive problems, however, is more appropriate to use for classification tasks. \n",
    "kNN is most useful with labelled data and it can achieve high accuracy in a wide variety of prediction-type problems. It‚Äôs also used in many other different areas, such as image and video recognition, handwriting detection, but also to search semantically similar documents.\n",
    "This specific study from Musa, O., Malik, A., Buna, I., Lasena, M., Rizal Isnanto, R. and Suryono, S. (2018) focuses on the application of the K-Nearest Neighbour algorithm in the expert system for diagnosing lung disease, according to its type.\n",
    "The dataset comprises of 12 types of lung disease and 133 observations and the results of the predictions carried out by the kNN, produce a high accuracy of 96.97% and thus making it able to detect accurately lung disease. \n",
    "kNN is an appropriate model for this use-case, because it is a classification problem with labelled data (type of lung diseases) .\n",
    "The dataset is also fairly small with lower dimensionality making kNN the most suitable algorithm.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES:\n",
    "\n",
    " - Tiwari, M. (2020). Iris Data set Analysis using KNN - Analytics Vidhya - Medium. [online] Medium. Available at: https://medium.com/analytics-vidhya/iris-data-set-analysis-using-knn-bfea147423ee [Accessed 13 Feb. 2022].\n",
    " \n",
    "- scikit-learn. (2021). Feature importances with a forest of trees. [online] Available at: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html [Accessed 13 Feb. 2022].\n",
    "\n",
    "\n",
    "- London.ac.uk. (2021). Course: Machine learning (DSM040-2021-OCT), Topic: Solutions to Jupyter Notebooks. [online] Available at: https://learn.london.ac.uk/course/view.php?id=585¬ßion=15 [Accessed 13 Feb. 2022].\n",
    "\n",
    "\n",
    "- Fernando, L. (2022). Case Study: Spam Detection With Naive Bayes. [online] Github.io. Available at: https://ljfernando.github.io/SpamDetectionNaiveBayes/ [Accessed 20 Feb. 2022].\n",
    "\n",
    "\n",
    "- Meena, M. (2020). Applications of Random Forest. [online] OpenGenus IQ: Computing Expertise & Legacy. Available at: https://iq.opengenus.org/applications-of-random-forest/ [Accessed 20 Feb. 2022].\n",
    "\n",
    "\n",
    "- MansiMeena (2013). GitHub - MansiMeena/Credit-Card-Fraud-Detection. [online] GitHub. Available at: https://github.com/MansiMeena/Credit-Card-Fraud-Detection [Accessed 4 Mar. 2022].\n",
    "\n",
    "\n",
    "- Musa, O., Malik, A., Buna, I., Lasena, M., Rizal Isnanto, R. and Suryono, S. (2018). Application Of K-Nearest Neighbor (K-NN) Algorithm On Lung Disease Diagnosis Expert System. International Journal of Scientific & Engineering Research, [online] 9. Available at: https://www.ijser.org/researchpaper/Application-Of-K-Nearest-Neighbor-K-NN-Algorithm-On-Lung-Disease-Diagnosis-Expert-System.pdf.\n",
    "\n",
    "\n",
    "- Fernando, L. (2022). Case Study: Spam Detection With Naive Bayes. [online] Github.io. Available at: https://ljfernando.github.io/SpamDetectionNaiveBayes/ [Accessed 4 Mar. 2022]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
